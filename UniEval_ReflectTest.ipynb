{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FELaSi8WWDYf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/DRAEM/UniEval/UniEval-main\n",
        "!pwd\n",
        "!pip install -r requirements.txt\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2jFBcEYpWRFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output must be True\n",
        "- view resources\n",
        "- Change runtime type\n",
        "- choose GPU"
      ],
      "metadata": {
        "id": "7-XTe9OaWXrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "fba9iK4vWTvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Reflect Dataset:\n",
        "# reflect dataset\n",
        "\n",
        "from utils import convert_to_json\n",
        "from metric.evaluator import get_evaluator\n",
        "\n",
        "task = 'dialogue'\n",
        "\n",
        "# a list of dialogue histories\n",
        "src_list = ['hi , do you know much about the internet ? \\n i know a lot about different sites and some website design , how about you ? \\n\\n']\n",
        "# a list of additional context that should be included into the generated response\n",
        "context_list = ['\"Bailey had a really long and stressful day at work and was exhausted when they came home.']\n",
        "# a list of model outputs to be evaluated\n",
        "output_list1 = [\"I knew when you told me about the crazy day you were having that a good meal would be take your mind off it.\"]\n",
        "output_list2 = [\"You looked drained when you got home. I wasn't sure what more I could do for you, so I just let you rest.\"]\n",
        "output_list3 = [\"I don't think I have ever seen you so tired as you were when you got back from work last night.\"]\n",
        "output_list = [\"I knew when you told me about the crazy day you were having that a good meal would be take your mind off it.\",\n",
        "            \"You looked drained when you got home. I wasn't sure what more I could do for you, so I just let you rest.\",\n",
        "            \"I don't think I have ever seen you so tired as you were when you got back from work last night.\"]\n",
        "\n",
        "# Prepare data for pre-trained evaluators\n",
        "data1 = convert_to_json(output_list=output_list1, \n",
        "                      src_list=src_list, context_list=context_list)\n",
        "# Initialize evaluator for a specific task\n",
        "evaluator = get_evaluator(task)\n",
        "# Get multi-dimensional evaluation scores\n",
        "eval_scores1 = evaluator.evaluate(data1, print_result=True)\n"
      ],
      "metadata": {
        "id": "JxrA_omKWvbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "from utils import convert_to_json\n",
        "from metric.evaluator import get_evaluator\n",
        "task = 'dialogue'\n",
        "with open(\"/content/drive/Shareddrives/DRAEM/Reflect/data/organized_Reflect_9k_responses.json\") as f:\n",
        "    c = json.load(f)\n",
        "    # print(c[2]['dialogue'])\n",
        "    #print(len(c))\n",
        "    myFile = open('/content/drive/Shareddrives/DRAEM/UniEval/Results/UniEval_Reflect_Results.csv', 'w')\n",
        "    writer = csv.writer(myFile)\n",
        "    writer.writerow(['dialogue', 'reaction', 'response','naturalness', 'coherence', 'engagingness', 'roundedness', 'understandability','overall'])    \n",
        "    for i in range(1,len(c))):\n",
        "      source_list = list(range(len(c)))\n",
        "      print(\"count:\", i)\n",
        "      #print(type(c[i]['dialogue']))\n",
        "      source_list[i] = c[i]['dialogue']\n",
        "      #print(\"dialogue: \", source_list[i])\n",
        "      for num in range(1, 6):\n",
        "          #print(\"num: \", num)\n",
        "          con_list = list(range(6))\n",
        "          #output_list = list(range(6))\n",
        "          con_list [num] = c[i]['reaction_{num}'.format(num = num)]\n",
        "          \n",
        "          #print(\"reaction_{}\".format(num) + \":\"  , con_list [num])\n",
        "          #print(\"responses_{}\".format(num) + \":\"  , output_list [num])\n",
        "          \n",
        "          for j in range(0, len(c[i]['responses_{num}'.format(num = num)])):\n",
        "              #print(\"length:\",len(c[i]['responses_{num}'.format(num = num)]) )\n",
        "              out_list = list(range(3))\n",
        "              out_list[j] = c[i]['responses_{num}'.format(num = num)][j]\n",
        "              #print(\"out_list{}\".format(j)+\":\",out_list[j] )\n",
        "              data = convert_to_json(output_list=[out_list[j]], src_list=[source_list[i]], context_list=[con_list[num]])\n",
        "      #         # Initialize evaluator for a specific task\n",
        "              evaluator = get_evaluator(task)\n",
        "      #         # Get multi-dimensional evaluation scores\n",
        "              dicts = [{'src_list': source_list[i] , 'context_list': 'reaction_'+ str(num), 'output_list': out_list[j]}]\n",
        "              #print(dicts)\n",
        "              eval_scores = evaluator.evaluate(data, print_result=False)\n",
        "              #print(\"eval_scores :\",eval_scores)\n",
        "              #print(\"type of eval_scores\",type(eval_scores))\n",
        "              for dic1 in dicts:\n",
        "                for dic2 in eval_scores:\n",
        "                  #print (dic1)\n",
        "                  #print (dic2)\n",
        "                  dic_new ={**dic1,**dic2 }\n",
        "                  results = [dic_new]\n",
        "              #writer.writer(out_list[j])\n",
        "              for dictionary in results:\n",
        "                writer.writerow( dictionary.values())\n",
        "    myFile.close()\n",
        "      # print('...')"
      ],
      "metadata": {
        "id": "hfOlorN0W5HQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}